# Design Document: Document Q&A System

## Overview

The Document Q&A System implements a Retrieval Augmented Generation (RAG) architecture that enables users to query PDF documents using natural language via REST API. The system consists of three main layers:

1. **API Layer**: FastAPI backend exposing REST endpoints with auto-generated Swagger UI for interactive testing
2. **Processing Layer**: Document processing, embedding generation, and RAG orchestration
3. **Storage Layer**: Vector database (ChromaDB/FAISS) and file storage

The architecture prioritizes local execution, privacy, and learning-friendly code organization. All AI operations run locally using Ollama for LLM inference and sentence-transformers for embeddings. FastAPI automatically provides Swagger UI at /docs for easy API testing and file uploads without requiring a separate frontend.

## Architecture

### System Components

```
┌─────────────────────────────────────────────────────────────┐
│                   FastAPI Backend + Swagger UI               │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  API Routes  │  │  Swagger UI  │  │  Health      │      │
│  │  /docs       │  │  /api/*      │  │  Checks      │      │
│  └──────┬───────┘  └──────┬───────┘  └──────────────┘      │
│         │                  │                                 │
│  ┌──────┴──────────────────┴───────┐                        │
│  │      RAG Engine                  │                        │
│  │  ┌────────────┐  ┌────────────┐ │                        │
│  │  │  Retriever │  │  Generator │ │                        │
│  │  └────────────┘  └────────────┘ │                        │
│  └──────┬───────────────┬──────────┘                        │
└─────────┼───────────────┼─────────────────────────────────┘
          │               │
┌─────────┴──────┐  ┌────┴──────────┐
│ Document       │  │  Embedding    │
│ Processor      │  │  Service      │
└────────┬───────┘  └───────┬───────┘
         │                  │
┌────────┴──────────────────┴───────┐  ┌──────────────┐
│      Vector Store                  │  │   Ollama     │
│  (ChromaDB or FAISS)               │  │   LLM        │
└────────────────────────────────────┘  └──────────────┘
```

### User Interaction Flow

Users interact with the system through Swagger UI at http://localhost:8000/docs:
- Upload PDF documents using the interactive file upload widget
- Submit questions via the /api/questions endpoint
- View responses with source citations
- Manage documents (list, delete) through API endpoints

### Data Flow

**Document Upload Flow:**
1. User uploads PDF via Swagger UI at /docs
2. Swagger UI sends multipart/form-data POST to `/api/documents`
3. Backend saves file and triggers Document_Processor
4. Document_Processor extracts text, chunks it, and sends to Embedding_Service
5. Embedding_Service generates vectors and stores in Vector_Store
6. Backend returns success response with document ID

**Question Answering Flow:**
1. User submits question via Swagger UI /api/questions endpoint
2. Backend generates question embedding via Embedding_Service
3. RAG_Engine queries Vector_Store for top-k similar chunks
4. RAG_Engine constructs prompt with retrieved context
5. LLM_Service sends prompt to Ollama via HTTP API
6. Backend returns complete answer with source citations in JSON response

## Components and Interfaces

### Backend API (FastAPI)

**API Endpoints**

```python
# Document Management
POST   /api/documents          # Upload document (multipart/form-data)
GET    /api/documents          # List all documents
GET    /api/documents/{id}     # Get document details
DELETE /api/documents/{id}     # Delete document

# Question Answering
POST   /api/questions          # Submit question and get answer

# Health and Status
GET    /api/health             # System health check
GET    /api/models             # List available Ollama models

# Interactive Documentation
GET    /docs                   # Swagger UI (auto-generated by FastAPI)
GET    /redoc                  # ReDoc alternative documentation
```

**Swagger UI Features:**
- Interactive API testing directly from browser
- File upload widget for testing document uploads
- Request/response examples for all endpoints
- Schema validation and error messages
- Try-it-out functionality for all endpoints

**Request/Response Models**

```python
class DocumentUploadResponse(BaseModel):
    document_id: str
    filename: str
    status: str
    page_count: int

class QuestionRequest(BaseModel):
    question: str
    document_ids: Optional[List[str]] = None  # Filter to specific docs
    max_chunks: int = 5
    temperature: float = 0.7

class QuestionResponse(BaseModel):
    answer: str
    sources: List[Source]
    model_used: str
    
class Source(BaseModel):
    document_id: str
    document_name: str
    page_number: int
    chunk_text: str
    similarity_score: float
```

### Document Processor

**Responsibilities:**
- Extract text from PDFs using PyPDF2 or pdfplumber
- Split text into overlapping chunks
- Preserve metadata (page numbers, positions)
- Handle various PDF formats and encodings

**Interface:**

```python
class DocumentProcessor:
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def process_document(self, file_path: str) -> ProcessedDocument:
        """Extract and chunk document text"""
        pass
    
    def extract_text(self, file_path: str) -> List[Page]:
        """Extract text from PDF pages"""
        pass
    
    def chunk_text(self, pages: List[Page]) -> List[Chunk]:
        """Split text into overlapping chunks"""
        pass

class ProcessedDocument:
    document_id: str
    filename: str
    chunks: List[Chunk]
    page_count: int

class Chunk:
    chunk_id: str
    text: str
    page_number: int
    chunk_index: int
    metadata: Dict[str, Any]
```

**Chunking Strategy:**
- Use recursive character text splitter
- Target chunk size: 1000 characters (~250 tokens)
- Overlap: 100 characters to maintain context
- Split on sentence boundaries when possible
- Preserve paragraph structure in metadata

### Embedding Service

**Responsibilities:**
- Load and manage sentence-transformers model
- Generate embeddings for document chunks and questions
- Batch processing for efficiency
- Cache model in memory

**Interface:**

```python
class EmbeddingService:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
    
    def embed_text(self, text: str) -> np.ndarray:
        """Generate embedding for single text"""
        pass
    
    def embed_batch(self, texts: List[str]) -> np.ndarray:
        """Generate embeddings for multiple texts"""
        pass
    
    def get_embedding_dimension(self) -> int:
        """Return embedding vector dimension"""
        pass
```

**Model Selection:**
- Default: `all-MiniLM-L6-v2` (384 dimensions, fast, good quality)
- Alternative: `all-mpnet-base-v2` (768 dimensions, higher quality, slower)
- Configurable via environment variable

### Vector Store

**Responsibilities:**
- Store and index document chunk embeddings
- Perform similarity search
- Manage document metadata
- Persist data to disk

**Interface:**

```python
class VectorStore(ABC):
    @abstractmethod
    def add_documents(self, chunks: List[Chunk], embeddings: np.ndarray):
        """Store chunks with their embeddings"""
        pass
    
    @abstractmethod
    def search(self, query_embedding: np.ndarray, top_k: int = 5, 
               document_ids: Optional[List[str]] = None) -> List[SearchResult]:
        """Find most similar chunks"""
        pass
    
    @abstractmethod
    def delete_document(self, document_id: str):
        """Remove all chunks for a document"""
        pass
    
    @abstractmethod
    def get_document_count(self) -> int:
        """Return total number of indexed documents"""
        pass

class SearchResult:
    chunk: Chunk
    similarity_score: float
```

**ChromaDB Implementation:**
```python
class ChromaVectorStore(VectorStore):
    def __init__(self, persist_directory: str = "./chroma_db"):
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = self.client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )
```

**FAISS Implementation:**
```python
class FAISSVectorStore(VectorStore):
    def __init__(self, index_path: str = "./faiss_index"):
        self.index_path = index_path
        self.index = None  # Loaded on first use
        self.metadata_store = {}  # Separate metadata storage
```

### RAG Engine

**Responsibilities:**
- Orchestrate retrieval and generation
- Construct prompts with retrieved context
- Format sources and citations
- Handle streaming responses

**Interface:**

```python
class RAGEngine:
    def __init__(self, vector_store: VectorStore, 
                 embedding_service: EmbeddingService,
                 llm_service: LLMService):
        self.vector_store = vector_store
        self.embedding_service = embedding_service
        self.llm_service = llm_service
    
    async def answer_question(self, question: str, 
                              document_ids: Optional[List[str]] = None,
                              max_chunks: int = 5) -> str:
        """Generate answer and return complete response"""
        pass
    
    def retrieve_context(self, question: str, 
                        document_ids: Optional[List[str]] = None,
                        max_chunks: int = 5) -> List[SearchResult]:
        """Retrieve relevant chunks"""
        pass
    
    def construct_prompt(self, question: str, 
                        context_chunks: List[SearchResult]) -> str:
        """Build prompt with context"""
        pass
```

**Prompt Template:**

```python
PROMPT_TEMPLATE = """You are a helpful assistant that answers questions based on the provided context from documents.

Context from documents:
{context}

Question: {question}

Instructions:
- Answer the question based solely on the provided context
- If the context doesn't contain enough information, say so
- Be concise and accurate
- Cite specific parts of the context when relevant

Answer:"""
```

### LLM Service

**Responsibilities:**
- Interface with Ollama HTTP API
- Support both streaming and non-streaming modes
- Handle connection errors and retries
- Configure generation parameters

**Interface:**

```python
class LLMService:
    def __init__(self, base_url: str = "http://localhost:11434",
                 model_name: str = "llama2"):
        self.base_url = base_url
        self.model_name = model_name
    
    async def generate(self, prompt: str, 
                      temperature: float = 0.7,
                      max_tokens: int = 512,
                      stream: bool = False) -> Union[str, AsyncIterator[str]]:
        """Generate response with optional streaming"""
        pass
    
    async def check_health(self) -> bool:
        """Check if Ollama is accessible"""
        pass
    
    async def list_models(self) -> List[str]:
        """Get available models"""
        pass
```

**Ollama API Integration:**
```python
# POST to http://localhost:11434/api/generate
{
    "model": "llama2",
    "prompt": "...",
    "stream": false,  # Set to true for streaming responses
    "options": {
        "temperature": 0.7,
        "num_predict": 512
    }
}
```

## Data Models

### Database Schema (Metadata)

**Documents Table** (SQLite for metadata)
```sql
CREATE TABLE documents (
    id TEXT PRIMARY KEY,
    filename TEXT NOT NULL,
    file_path TEXT NOT NULL,
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    page_count INTEGER,
    chunk_count INTEGER,
    status TEXT CHECK(status IN ('processing', 'ready', 'error')),
    error_message TEXT
);
```

**Chunks Metadata** (stored in Vector Store)
```python
{
    "chunk_id": "doc123_chunk_5",
    "document_id": "doc123",
    "document_name": "example.pdf",
    "page_number": 3,
    "chunk_index": 5,
    "text": "...",
    "char_count": 987
}
```

### File Storage Structure

```
data/
├── documents/           # Original PDF files
│   ├── doc123.pdf
│   └── doc456.pdf
├── chroma_db/          # ChromaDB persistent storage
│   └── ...
├── faiss_index/        # FAISS index files (if using FAISS)
│   ├── index.faiss
│   └── metadata.json
└── metadata.db         # SQLite database for document metadata
```

## Correctness Properties

*A property is a characteristic or behavior that should hold true across all valid executions of a system—essentially, a formal statement about what the system should do. Properties serve as the bridge between human-readable specifications and machine-verifiable correctness guarantees.*


### Property Reflection

After analyzing all acceptance criteria, I've identified the following consolidations to eliminate redundancy:

**Consolidations:**
- Properties 1.4 and 1.5 (embedding generation and persistence) can be combined into a single round-trip property
- Properties 7.1-7.5 (API endpoint existence) can be consolidated into a single API contract property
- Properties 10.3 and 10.4 (specific error logging) are covered by the general property 10.1
- Properties 11.2-11.5 (individual configuration options) can be consolidated into a general configuration property
- Property 12.3 subsumes the deletion aspects of property 3.5

**Unique Properties Retained:**
- File size validation (1.1)
- Text extraction completeness (1.2)
- Chunking constraints (1.3, 9.1, 9.2)
- Embedding dimension consistency (5.3)
- Vector store persistence (3.4)
- Error handling and responses (1.7, 7.6, 11.7)
- Document ID uniqueness (12.1)
- Multi-document search (12.5)

### Correctness Properties

**Property 1: File size validation**
*For any* file upload request, files under 50MB should be accepted and files over 50MB should be rejected with an appropriate error message.
**Validates: Requirements 1.1**

**Property 2: Complete text extraction**
*For any* valid PDF document, all pages should be processed and text should be extracted from each page.
**Validates: Requirements 1.2**

**Property 3: Chunk size constraints**
*For any* document text, all generated chunks should have token counts between 500 and 1000 tokens (inclusive).
**Validates: Requirements 1.3**

**Property 4: Chunk overlap preservation**
*For any* two consecutive chunks from the same document, there should be approximately 100 tokens of overlapping content.
**Validates: Requirements 1.3, 9.2**

**Property 5: Sentence boundary preservation**
*For any* generated chunk, the chunk should not end in the middle of a sentence (should end with sentence-ending punctuation or at document boundary).
**Validates: Requirements 9.1**

**Property 6: Embedding generation round-trip**
*For any* document chunks, after generating embeddings and storing them in the Vector_Store, retrieving them should return the same chunks with their original metadata intact.
**Validates: Requirements 1.4, 1.5**

**Property 7: Question embedding generation**
*For any* question text, the Embedding_Service should generate a valid embedding vector.
**Validates: Requirements 2.1**

**Property 8: Retrieval count accuracy**
*For any* question embedding, when the Vector_Store contains at least 5 chunks, the search should return exactly 5 results; when fewer than 5 chunks exist, it should return all available chunks.
**Validates: Requirements 2.2**

**Property 9: Prompt construction completeness**
*For any* question and set of retrieved chunks, the constructed prompt should contain both the original question text and all retrieved chunk texts.
**Validates: Requirements 2.3**

**Property 10: Source citation inclusion**
*For any* generated answer, the response should include source citations containing document ID, document name, and page number for each retrieved chunk.
**Validates: Requirements 2.7**

**Property 11: Embedding dimension consistency**
*For any* set of text inputs (chunks or questions), all generated embeddings should have identical vector dimensions.
**Validates: Requirements 5.3**

**Property 12: Vector store persistence**
*For any* set of embeddings stored in the Vector_Store, after a system restart, querying with the same embedding should return the same results.
**Validates: Requirements 3.4**

**Property 13: Complete document deletion**
*For any* document, after deletion, the Vector_Store should contain zero chunks associated with that document ID, and the document file should no longer exist.
**Validates: Requirements 3.5, 12.3**

**Property 14: Error message descriptiveness**
*For any* invalid PDF file or processing error, the system should return an error response containing a non-empty, human-readable error message.
**Validates: Requirements 1.7**

**Property 15: API error response format**
*For any* API error condition, the response should include an appropriate HTTP status code (4xx or 5xx) and a JSON body containing an error message field.
**Validates: Requirements 7.6**

**Property 16: Configuration validation**
*For any* invalid configuration value (negative chunk size, non-existent model name, etc.), the system should fail to start and provide a clear error message indicating which configuration is invalid.
**Validates: Requirements 11.7**

**Property 17: Document ID uniqueness**
*For any* set of uploaded documents, all document IDs should be unique (no two documents should share the same ID).
**Validates: Requirements 12.1**

**Property 18: Document metadata completeness**
*For any* document listing response, each document entry should include filename, upload date, and page count fields.
**Validates: Requirements 12.2**

**Property 19: Multi-document search**
*For any* question when multiple documents are uploaded, the search should be capable of returning chunks from different documents in the same result set.
**Validates: Requirements 12.5**

**Property 20: Request logging completeness**
*For any* API request, a log entry should be created containing the timestamp, endpoint, HTTP method, and response status code.
**Validates: Requirements 10.1, 10.2**

**Property 21: Chunk metadata preservation**
*For any* document chunk, the stored metadata should include page number, chunk index, and document ID.
**Validates: Requirements 9.3**

## Error Handling

### Error Categories

**1. Document Processing Errors**
- Invalid PDF format or corrupted files
- Password-protected PDFs
- PDFs with no extractable text (scanned images)
- File size exceeds limit
- Unsupported file types

**Error Response:**
```python
{
    "error": "document_processing_error",
    "message": "Failed to extract text from PDF: file may be corrupted",
    "document_id": "doc123",
    "details": {
        "filename": "example.pdf",
        "error_type": "PDFParseError"
    }
}
```

**2. Vector Store Errors**
- Database connection failures
- Disk space exhausted
- Index corruption
- Query timeout

**Error Response:**
```python
{
    "error": "vector_store_error",
    "message": "Failed to store embeddings: disk space exhausted",
    "retry_possible": false
}
```

**3. LLM Service Errors**
- Ollama not running or unreachable
- Model not found
- Generation timeout
- Context length exceeded

**Error Response:**
```python
{
    "error": "llm_service_error",
    "message": "Ollama service is not accessible at http://localhost:11434",
    "details": {
        "service_url": "http://localhost:11434",
        "suggestion": "Ensure Ollama is running: docker-compose up ollama"
    }
}
```

**4. Embedding Service Errors**
- Model download failure
- Out of memory
- Invalid input text

**Error Response:**
```python
{
    "error": "embedding_error",
    "message": "Failed to generate embeddings: model not loaded",
    "retry_possible": true
}
```

**5. API Validation Errors**
- Missing required fields
- Invalid parameter types
- Malformed requests

**Error Response:**
```python
{
    "error": "validation_error",
    "message": "Invalid request parameters",
    "details": [
        {
            "field": "question",
            "error": "Field is required"
        }
    ]
}
```

### Error Handling Strategy

**Retry Logic:**
- Transient errors (network, timeout): Retry up to 3 times with exponential backoff
- Permanent errors (invalid file, missing model): Fail immediately with clear message
- Partial failures (some chunks failed): Process successful chunks, log failures

**Graceful Degradation:**
- If LLM service is down: Return retrieved chunks without generation
- If vector store is slow: Reduce retrieval count, add timeout
- If embedding service fails: Queue for retry, notify user of delay

**Logging:**
- All errors logged with full stack traces at ERROR level
- Include request context (user ID, document ID, timestamp)
- Structured logging for easy parsing and monitoring

## Testing Strategy

### Dual Testing Approach

The system requires both unit tests and property-based tests for comprehensive coverage:

**Unit Tests:**
- Specific examples demonstrating correct behavior
- Edge cases (empty documents, single-page PDFs, special characters)
- Error conditions (corrupted files, service unavailable)
- Integration points between components
- API endpoint contracts

**Property-Based Tests:**
- Universal properties that hold for all inputs
- Comprehensive input coverage through randomization
- Minimum 100 iterations per property test
- Each test tagged with: **Feature: document-qa-system, Property {number}: {property_text}**

### Property-Based Testing Configuration

**Library Selection:**
- Python: Use `hypothesis` library for property-based testing
- Each correctness property should be implemented as a single property-based test
- Configure tests to run minimum 100 iterations
- Use appropriate strategies for generating test data (text, PDFs, embeddings)

**Test Organization:**
```python
# Example property test structure
from hypothesis import given, strategies as st

@given(st.text(min_size=1000, max_size=5000))
def test_chunk_size_constraints(document_text):
    """
    Feature: document-qa-system, Property 3: Chunk size constraints
    For any document text, all generated chunks should have token counts 
    between 500 and 1000 tokens.
    """
    processor = DocumentProcessor(chunk_size=1000, chunk_overlap=100)
    chunks = processor.chunk_text(document_text)
    
    for chunk in chunks:
        token_count = len(chunk.text.split())
        assert 500 <= token_count <= 1000
```

### Test Coverage Requirements

**Backend Components:**
- Document Processor: 90%+ coverage
- Embedding Service: 85%+ coverage
- Vector Store implementations: 85%+ coverage
- RAG Engine: 90%+ coverage
- LLM Service: 80%+ coverage (mocked Ollama)
- API endpoints: 95%+ coverage

**API Testing:**
- Endpoint contract tests
- Request validation tests
- Error response tests
- Swagger UI accessibility tests

### Integration Testing

**End-to-End Flows:**
1. Upload document → Process → Store → Query → Generate answer
2. Upload multiple documents → Query across all → Verify sources
3. Delete document → Verify cleanup → Query should not return deleted content
4. Error scenarios → Verify appropriate error messages

**Docker Integration:**
- Test docker-compose startup
- Verify service connectivity
- Test volume persistence
- Health check validation

### Performance Testing

**Benchmarks:**
- Document processing: < 5 seconds per MB
- Embedding generation: < 100ms per chunk
- Vector search: < 500ms for 10,000 chunks
- End-to-end query: < 3 seconds (excluding LLM generation time)

**Load Testing:**
- Concurrent uploads: 5 simultaneous uploads
- Concurrent queries: 10 simultaneous queries
- Large documents: 100+ page PDFs
- Large collections: 1000+ documents

### Testing Tools

**Backend:**
- pytest: Test framework
- hypothesis: Property-based testing
- pytest-asyncio: Async test support
- pytest-cov: Coverage reporting
- httpx: API testing
- pytest-mock: Mocking

**Docker:**
- docker-compose for test environment
- Testcontainers: Container-based integration tests (optional)

### Continuous Testing

**Pre-commit:**
- Run unit tests
- Run linting (black, flake8, mypy)
- Type checking

**CI Pipeline:**
- Run all unit tests
- Run property-based tests
- Run integration tests
- Generate coverage reports
- Build Docker images
- Run E2E tests in Docker environment

## Implementation Notes

### Technology Stack Summary

**Backend:**
- Python 3.10+
- FastAPI framework (with auto-generated Swagger UI)
- Pydantic for data validation
- SQLite for metadata
- python-multipart for file uploads
- uvicorn for ASGI server

**AI/ML:**
- Ollama (Docker container)
- sentence-transformers
- ChromaDB or FAISS
- PyPDF2 or pdfplumber for PDF parsing

**Infrastructure:**
- Docker & Docker Compose

### Development Phases

**Phase 1: Core Backend (MVP)**
- Document upload and storage
- PDF text extraction
- Basic chunking
- Embedding generation
- Vector store integration (ChromaDB)
- Simple Q&A endpoint

**Phase 2: RAG Enhancement**
- Improved chunking with overlap
- Prompt engineering
- Source citation
- Error handling
- Logging

**Phase 3: API Polish**
- Swagger UI customization
- Comprehensive API documentation
- Request validation
- Response formatting

**Phase 4: Production Ready**
- Docker containerization
- Configuration management
- Comprehensive testing
- Documentation
- Performance optimization

### Learning Checkpoints

As this is a learning project for a React + Node.js developer exploring Gen AI with Python:

1. **After Phase 1**: Understand embeddings, vector similarity, basic RAG, Python async/await
2. **After Phase 2**: Understand prompt engineering, chunking strategies, LLM integration
3. **After Phase 3**: Understand FastAPI features, Pydantic models, API design patterns
4. **After Phase 4**: Understand Docker deployment, production considerations

Focus areas for learning:
- How embeddings represent semantic meaning
- Vector similarity search mechanics
- RAG architecture and prompt construction
- Local LLM inference with Ollama
- Python type hints and Pydantic validation
- FastAPI's automatic documentation generation

### Configuration Examples

**Environment Variables (.env):**
```bash
# Backend
BACKEND_PORT=8000
UPLOAD_DIR=./data/documents
MAX_FILE_SIZE_MB=50

# Vector Store
VECTOR_STORE_TYPE=chromadb  # or faiss
CHROMA_PERSIST_DIR=./data/chroma_db
FAISS_INDEX_PATH=./data/faiss_index

# Embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu  # or cuda

# LLM
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama2
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=512

# Chunking
CHUNK_SIZE=1000
CHUNK_OVERLAP=100
MAX_CHUNKS_FOR_CONTEXT=5

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/app.log
```

**Docker Compose Structure:**
```yaml
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - ollama
  
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

volumes:
  ollama_data:
```

### Security Considerations

**File Upload Security:**
- Validate file types (only PDF)
- Scan for malicious content
- Limit file sizes
- Sanitize filenames
- Store files outside web root

**API Security:**
- Input validation on all endpoints
- Rate limiting
- CORS configuration
- No sensitive data in logs
- Secure file paths (prevent directory traversal)

**Docker Security:**
- Run containers as non-root user
- Limit container resources
- Use specific image versions (not :latest in production)
- Scan images for vulnerabilities

### Future Enhancements

**Potential Features:**
- Support for more document types (DOCX, TXT, HTML, Markdown)
- Custom frontend application (React, Vue, or mobile app)
- Multi-user support with authentication
- Document collections/folders
- Advanced search filters
- Export conversation history
- Fine-tuning embeddings for domain-specific documents
- GPU acceleration for embeddings
- Distributed vector store for scaling
- Web scraping for URL-based documents
- Streaming responses via Server-Sent Events (SSE)
